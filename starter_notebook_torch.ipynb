{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476a856",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "1634f85b",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -Uq wandb tqdm torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e30b90f",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "728881d8",
     "kernelId": "",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "API Key:  ········································\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "key = os.environ.get(\"WANDB_API_KEY\", None)\n",
    "if not key:\n",
    "    key = getpass.getpass(\"API Key: \")\n",
    "    !export WANDB_API_KEY={key}\n",
    "!wandb login {key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd0c8b1",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6372256b",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "run = wandb.init(entity=\"adrishd\", project=\"taco-baseline\")\n",
    "artifact = run.use_artifact('adrishd/taco/taco:pytorch', type='dataset')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7476c2",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "da2bbbb8",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "import tacoloader\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torchsummary\n",
    "import time\n",
    "import tqdm\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ef6d6",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "bad56fb3",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "h, w, c = 512, 512, 3\n",
    "transform = torchvision.transforms.Resize(\n",
    "    (h, w),\n",
    "    torchvision.transforms.InterpolationMode.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff7973a",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "9393e7f7",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset, collate_fn = tacoloader.load_dataset(artifact_dir, tacoloader.Environment.TORCH, transform_fn=transform)\n",
    "split = 0.8\n",
    "dataset_size = len(dataset)\n",
    "indices = range(dataset_size)\n",
    "train_indices = indices[:int(split * dataset_size)]\n",
    "test_indices = indices[int(split * dataset_size) + 1:]\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b866829e",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "02387601",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=10,\n",
    "    collate_fn=dataset.collate_fn,\n",
    "    num_workers=6,\n",
    "    shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=dataset.collate_fn,\n",
    "    num_workers=6,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_mask(pred_mask, true_mask):\n",
    "    pred_labels = torch.unique(pred_mask).cpu().numpy().tolist()\n",
    "    predicted_class_labels = {\n",
    "        i : x for i, x in enumerate(dataset.get_categories(pred_labels))\n",
    "    }\n",
    "    gt_labels = torch.unique(true_mask).cpu().numpy().tolist()\n",
    "    ground_truth_labels = {\n",
    "        i: x for i, x in enumerate(dataset.get_categories(gt_labels))\n",
    "    }\n",
    "    wandb_image = wandb.Image(sample.image, masks={\n",
    "        \"prediction\": {\n",
    "            \"mask_data\": pred_mask.squeeze(),\n",
    "            \"class_labels\": predicted_class_labels\n",
    "        },\n",
    "        \"ground_truth\": {\n",
    "            \"mask_data\": sample.segmentation,\n",
    "            \"class_labels\": ground_truth_labels\n",
    "        }\n",
    "    })\n",
    "    wandb.log(\"semantic_segmentation\" : wandb_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d53b1bc",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "4fbc8a46",
     "kernelId": ""
    }
   },
   "source": [
    "## Model Design and Implementations\n",
    "### Starter Code: Helper Modules for UNet Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185adae",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "432f3df7",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/xiaopeng-liao/Pytorch-UNet/blob/master/unet/unet_parts.py\n",
    "class double_conv(torch.nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(out_ch),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(out_ch),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = torch.nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffX = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        #print('sizes',x1.size(),x2.size(),diffX // 2, diffX - diffX//2, diffY // 2, diffY - diffY//2)\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    "                        diffY // 2, diffY - diffY//2))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_ch, out_ch, 1)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35e53c",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f53a7046",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 16)\n",
    "        self.down1 = down(16, 32)\n",
    "        self.down2 = down(32, 64)\n",
    "        self.down3 = down(64, 128)\n",
    "        self.down4 = down(128, 128)\n",
    "        self.up1 = up(256, 64, bilinear=False)\n",
    "        self.up2 = up(128, 32, bilinear=False)\n",
    "        self.up3 = up(64, 16, bilinear=False)\n",
    "        self.up4 = up(32, 16, bilinear=False)\n",
    "        self.outc = outconv(16, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28791c2",
   "metadata": {
    "gradient": {
     "id": "d72b2488",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "unet = UNet(3, dataset.len_categories).cuda()\n",
    "torchsummary.summary(unet, (c, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f1c19d",
   "metadata": {
    "gradient": {
     "id": "83ade712",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(unet.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eca030",
   "metadata": {},
   "source": [
    "### Training and Logging Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff55b6",
   "metadata": {
    "gradient": {
     "id": "93b7c0b2",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "bar = tqdm.tqdm(train_loader)\n",
    "for data in bar:\n",
    "    optim.zero_grad()\n",
    "    segmask = unet(data.images.cuda())\n",
    "    loss = loss_fn(segmask, data.masks.cuda().long())\n",
    "    loss.backward()\n",
    "    mask = torch.argmax(segmask[0:1], dim=1).detach()\n",
    "    viz_mask(mask, sample.mask)\n",
    "    bar.set_description(\"Loss: %f\" % loss.detach().cpu())\n",
    "    wandb.log({\"loss\": loss.detach().cpu()})\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

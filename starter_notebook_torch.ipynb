{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41696077",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "d0d8b5d4",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -Uq wandb tqdm torchsummary\n",
    "!pip install -Uq randomname # for generating funky names for the sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac577d7",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "69990fd2",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "# Define wandb username, project name and dataset path\n",
    "wandb_username = \"adrishd\"\n",
    "wandb_project = \"taco-baseline\"\n",
    "dataset_artifact = 'adrishd/taco/taco:pytorch'\n",
    "\n",
    "# Downloading dataset\n",
    "# use root parameter in artifacts.download(root=<custom_path>)\n",
    "# to specify download directory. else download in the current directory.\n",
    "with wandb.init(entity=wandb_username, project=wandb_project) as run:\n",
    "    artifact = run.use_artifact(dataset_artifact, type='dataset')\n",
    "    artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cedfcc6",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "775437a6",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "outputs": [],
   "source": [
    "import randomname\n",
    "import tacoloader\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torchsummary\n",
    "import time\n",
    "import tqdm as tqdm\n",
    "import torchvision\n",
    "\n",
    "device = (\"cuda:0\" if torch.cuda.is_available() else \"cpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ede5d9",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "3ef66ee1",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "outputs": [],
   "source": [
    "h, w, c = 512, 512, 3 # height, width and channel of images\n",
    "# Use torchvision.transpose.Compose to compose multiple transformations\n",
    "# together. Refer to: https://pytorch.org/vision/stable/transforms.html\n",
    "transform = torchvision.transforms.Resize(\n",
    "        (h, w),\n",
    "        torchvision.transforms.InterpolationMode.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc2b08",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6b2d2a07",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "outputs": [],
   "source": [
    "# Constants in the training pipeline\n",
    "train_batch_size = 10\n",
    "test_batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb11c2",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "115efb09",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "outputs": [],
   "source": [
    "# By default an LRU cache is used for storing the last recently loaded image\n",
    "# and re-using that in the next epoch.\n",
    "# To store `all` the loaded images in memory and reuse them in the next epochs\n",
    "# set cache_fn=taco_loader.cache_fn in tacoloader.load_dataset(...)\n",
    "# an user can also increase the size of LRU cache, by\n",
    "# setting cache_fn=functools.lru_cache(maxsize=<int size>) in\n",
    "# tacoloader.load_dataset(...)\n",
    "\n",
    "dataset, collate_fn = tacoloader.load_dataset(\n",
    "  artifact_dir,\n",
    "  tacoloader.Environment.TORCH,\n",
    "  transform_fn=transform\n",
    ")\n",
    "\n",
    "# Splitting Dataset to 80%-20% for training and testing, respectively\n",
    "train_split = 0.8\n",
    "dataset_size = len(dataset)\n",
    "indices = range(dataset_size)\n",
    "train_indices = indices[:int(train_split * dataset_size)]\n",
    "test_indices = indices[int(train_split * dataset_size) + 1:]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b6aee",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "3e09efda",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "outputs": [],
   "source": [
    "# Creating Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    collate_fn=dataset.collate_fn,\n",
    "    num_workers=6,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=test_batch_size,\n",
    "    collate_fn=dataset.collate_fn,\n",
    "    num_workers=6,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdfb68a",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "eee8b1da",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "outputs": [],
   "source": [
    "def viz_mask(image, pred_mask, true_mask):\n",
    "    # Visualize segmentation mask on W&B dashboard\n",
    "    # image: torch tensor of dim [c, h, w]\n",
    "    # pred_mask: detached torch tensor of dim [h, w]\n",
    "    # true_mask: torch tensor of dim [h, w]\n",
    "    pred_labels = torch.unique(pred_mask).cpu().numpy().tolist()\n",
    "    predicted_class_labels = {\n",
    "        i : x for i, x in enumerate(dataset.get_categories(pred_labels))\n",
    "    }\n",
    "    gt_labels = torch.unique(true_mask).cpu().numpy().tolist()\n",
    "    ground_truth_labels = {\n",
    "        i: x for i, x in enumerate(dataset.get_categories(gt_labels))\n",
    "    }\n",
    "    wandb_image = wandb.Image(image.cpu(), masks={\n",
    "        \"prediction\": {\n",
    "            \"mask_data\": pred_mask.squeeze().cpu().numpy(),\n",
    "            \"class_labels\": predicted_class_labels\n",
    "        },\n",
    "        \"ground_truth\": {\n",
    "            \"mask_data\": true_mask.cpu().numpy(),\n",
    "            \"class_labels\": ground_truth_labels\n",
    "        }\n",
    "    })\n",
    "    wandb.log({\"semantic_segmentation\" : wandb_image})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b9880",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "c0450e91",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "source": [
    "## Model Design and Implementations\n",
    "### Starter Code: Helper Modules for UNet Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9104656",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "981805e0",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function for getting activation functions\n",
    "# from torch.nn given the function name.\n",
    "# activations with inplace operations, are enabled\n",
    "# by default.\n",
    "import inspect\n",
    "import functools\n",
    "def get_activation_fn(fn_name):\n",
    "    fn = getattr(torch.nn, fn_name)\n",
    "    isinplace = \"inplace\" in inspect.signature(fn).parameters\n",
    "    if isinplace:\n",
    "        fn = functools.partial(fn, inplace=True)\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacbfd4",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "e7562ca0",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "outputs": [],
   "source": [
    "# Dummy baseline UNet model based on:\n",
    "# https://github.com/xiaopeng-liao/Pytorch-UNet/blob/master/unet/unet_parts.py\n",
    "class double_conv(torch.nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch, activation_fn_name):\n",
    "        super(double_conv, self).__init__()\n",
    "        activation_fn = get_activation_fn(activation_fn_name)\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(out_ch),\n",
    "            activation_fn(),\n",
    "            torch.nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            torch.nn.BatchNorm2d(out_ch),\n",
    "            activation_fn()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation_fn):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch, activation_fn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation_fn):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch, activation_fn)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation_fn, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "        if bilinear:\n",
    "            self.up = torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = torch.nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch, activation_fn)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffX = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    "                        diffY // 2, diffY - diffY//2))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(torch.nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_ch, out_ch, 1)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983a33a",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "586b1a29",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, config):\n",
    "        super(UNet, self).__init__()\n",
    "        mid_channels = config[\"unet_channels\"]\n",
    "        activation_fn = config[\"activation_fn\"]\n",
    "        self.inc = inconv(n_channels,\n",
    "                          mid_channels,\n",
    "                          activation_fn)\n",
    "        self.down1 = down(mid_channels,\n",
    "                          mid_channels * 2,\n",
    "                          activation_fn)\n",
    "        self.down2 = down(mid_channels * 2,\n",
    "                          mid_channels * 4,\n",
    "                          activation_fn)\n",
    "        self.down3 = down(mid_channels * 4,\n",
    "                          mid_channels * 8,\n",
    "                          activation_fn)\n",
    "        self.down4 = down(mid_channels * 8,\n",
    "                          mid_channels * 8,\n",
    "                          activation_fn)\n",
    "        self.up1 = up(mid_channels * 16,\n",
    "                      mid_channels * 4,\n",
    "                      activation_fn,\n",
    "                      bilinear=config[\"bilinear\"])\n",
    "        self.up2 = up(mid_channels * 8,\n",
    "                      mid_channels * 2,\n",
    "                      activation_fn,\n",
    "                      bilinear=config[\"bilinear\"])\n",
    "        self.up3 = up(mid_channels * 4,\n",
    "                      mid_channels,\n",
    "                      activation_fn,\n",
    "                      bilinear=config[\"bilinear\"])\n",
    "        self.up4 = up(mid_channels * 2,\n",
    "                      mid_channels,\n",
    "                      activation_fn,\n",
    "                      bilinear=config[\"bilinear\"])\n",
    "        self.outc = outconv(mid_channels, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b6c88",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "1b7bb80c",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af33291",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "108c7cd5",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "source": [
    "### Training, Logging, Finding Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc84ddf",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "a3dfa328",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "outputs": [],
   "source": [
    "# Using wandb's hyperparameter optimization framework sweeps\n",
    "# More information can be found here: https://docs.wandb.ai/guides/sweeps\n",
    "sweep_name = randomname.get_name()\n",
    "sweep_config = {\n",
    "    \"name\" : sweep_name,\n",
    "    \"method\" : \"bayes\",\n",
    "    \"metric\" : {\n",
    "        \"name\": \"mean_iou\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\" : {\n",
    "        \"epochs\" : {\n",
    "            \"values\": [0, 1, 3, 5]\n",
    "        },\n",
    "        \"lr\" : {\n",
    "            \"min\": 1e-4,\n",
    "            \"max\": 1e-2\n",
    "        },\n",
    "        \"activation_fn\" : {\n",
    "            \"values\" : [\"ReLU\", \"LeakyReLU\", \"PReLU\"]\n",
    "        },\n",
    "        \"unet_channels\" : {\n",
    "            \"values\" : [8, 16, 32]\n",
    "        },\n",
    "        \"bilinear\" : {\n",
    "            \"values\" : [True, False]\n",
    "        },\n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(\n",
    "    sweep_config,\n",
    "    entity=wandb_username,\n",
    "    project=wandb_project\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a1c4b",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "7994356c",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "outputs": [],
   "source": [
    "# accuracy metrics\n",
    "\n",
    "def pixel_accuracy(pred, ground):\n",
    "    eqmap = torch.eq(pred, ground).int()\n",
    "    return eqmap.sum().float() / eqmap.numel()\n",
    "\n",
    "# mIOU based on: https://stackoverflow.com/questions/62461379/multiclass-semantic-segmentation-model-evaluation\n",
    "def mIOU(pred, label, num_classes):\n",
    "    iou_list = list()\n",
    "    present_iou_list = list()\n",
    "    pred = pred.view(-1)\n",
    "    label = label.view(-1)\n",
    "    for sem_class in range(num_classes):\n",
    "        pred_inds = (pred == sem_class)\n",
    "        target_inds = (label == sem_class)\n",
    "        if target_inds.long().sum() == 0:\n",
    "            iou_now = torch.nan\n",
    "        else: \n",
    "            intersection_now = (pred_inds[target_inds]).long().sum()\n",
    "            union_now = pred_inds.long().sum() + target_inds.long().sum() - intersection_now\n",
    "            iou_now = intersection_now.float() / union_now.float()\n",
    "            present_iou_list.append(iou_now)\n",
    "        iou_list.append(iou_now)\n",
    "    return torch.mean(torch.stack(present_iou_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc5bbb",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "638d18d7",
     "kernelId": "df2e43e8-9ac2-45ab-80f9-e4c0e060a469"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    with wandb.init(entity=wandb_username, project=wandb_project) as run:\n",
    "        config = wandb.config\n",
    "        num_classes = dataset.len_categories\n",
    "        unet = UNet(3, num_classes, config).to(device)\n",
    "        optim = torch.optim.Adam(unet.parameters(), lr=config[\"lr\"])\n",
    "        for x in tqdm.tqdm(range(config[\"epochs\"])):\n",
    "            bar = tqdm.tqdm(train_loader, leave=False)\n",
    "            for data in bar:\n",
    "                optim.zero_grad()\n",
    "                segmask = unet(data.images.to(device))\n",
    "                loss = loss_fn(segmask, data.masks.to(device).long())\n",
    "                loss.backward()\n",
    "                bar.set_description(\"Loss: %f\" % loss.detach().cpu())\n",
    "                wandb.log({\"loss\": loss.detach().cpu()})\n",
    "                optim.step()\n",
    "        test_bar = tqdm.tqdm(test_loader, position=0)\n",
    "        with torch.no_grad():\n",
    "            for data in test_bar:\n",
    "                segmask = unet(data.images.to(device))\n",
    "                mask = torch.argmax(segmask, dim=1).detach().cpu()\n",
    "                acc = pixel_accuracy(mask, data.masks)\n",
    "                miou = mIOU(mask, data.masks, num_classes).numpy()\n",
    "                test_bar.set_description(\"Acc: %.2f\" % acc)\n",
    "                wandb.log({\n",
    "                    \"mean_pixel_accuracy\": acc,\n",
    "                    \"mean_iou\": miou})\n",
    "        # Draw one sample and visualize the mask for each sweep\n",
    "        sample = test_dataset[0]\n",
    "        segmask = unet(sample.image.unsqueeze(0).to(device))\n",
    "        mask = torch.argmax(segmask, dim=1).detach().squeeze()\n",
    "        viz_mask(sample.image, mask, sample.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b97629",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 5 # Run 5 sweeps\n",
    "wandb.agent(\n",
    "    sweep_id,\n",
    "    function=train,\n",
    "    count=count\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
